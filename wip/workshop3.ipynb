{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop 3: List comprehensions and more useful functions\n",
    "#### What is a list comprehension?\n",
    "From W3Schools: \"List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list.\"\n",
    "\n",
    "List comprehensions can provide an alternative to loops, when you're trying to create a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Arma virumque canō, Trōiae quī prīmus ab ōrīs\\nĪtaliam, fātō profugus, Lāvīniaque vēnit\\nlītora, multum ille et terrīs iactātus et altō\\nvī superum saevae memorem Iūnōnis ob īram\"\n",
    "corpus_list = corpus.split()\n",
    "\n",
    "long_words = [word for word in corpus_list if len(word) > 4]\n",
    "print(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "long_words = [x for word in corpus_list if len(word) > 4]  \n",
    "Explanation: For each word in corpus_list, add the word to the new list long_words if that word's length is greater than 4\n",
    "\n",
    "You can also apply a function to whatever you're adding to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words = [word.upper() for word in corpus_list if len(word) > 4]\n",
    "print(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "long_words = [word.upper() for word in corpus_list if len(word) > 4]  \n",
    "Explanation:\n",
    "For each word in corpus_list, add the word in all caps to the new list long_words if that word's length is greater than 4\n",
    "\n",
    "Remember that list comprehensions can also be written as loops. This can sometimes be more readable, sometimes not, so you as the programmer can use your own discretion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words = [word.upper() for word in corpus_list if len(word) > 4]\n",
    "\n",
    "# the above code does the same thing as...\n",
    "\n",
    "long_words_2 = []\n",
    "for word in corpus_list:\n",
    "    if len(word) > 4:\n",
    "        long_words_2.append(word.upper())\n",
    "\n",
    "print(long_words)\n",
    "print(long_words_2)\n",
    "\n",
    "if long_words == long_words_2:\n",
    "    print(\"the lists are the same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List slicing\n",
    "List slicing lets you take a slice (or slices) of elements of a list in the format list[start : end : step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_list[:]) # all the elements\n",
    "print(corpus_list[0:5]) # first 5 elements, note that it includes the 0th element and not the 5th element\n",
    "print(corpus_list[4:10]) # a section in the middle (the 4th element up until and including the 9th element)\n",
    "print(corpus_list[::3]) # the whole list but only every 3rd element \n",
    "print(corpus_list[0:10:2]) # every other of the first 10 elements\n",
    "\n",
    "# and whatever else you need to do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some list functions\n",
    "To be honest, a lot of list functions such as all() map() and filter() can all be replaced with list comprehensions. You're welcome to check those out on your own, but I think that for our purposes it's easier to just use list comprehensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate\n",
    "corpus_list_nums = enumerate(corpus_list)\n",
    "for index, word in corpus_list_nums:\n",
    "    print(index, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min and max\n",
    "word_lengths = [3, 5, 2]\n",
    "longest_word_length = max(word_lengths)\n",
    "shortest_word_length = min(word_lengths)\n",
    "\n",
    "# but this is slightly strange data, let's see a different example:\n",
    "\n",
    "word_freqs = {\"hello\": 1, \"my\": 4, \"name\": 2, \"is\": 5}\n",
    "most_frequent = max(word_freqs.values())\n",
    "\n",
    "print(most_frequent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you programattically generate something like word_freqs?  \n",
    "In week 1 of workshop we talked about doing this with loops, but you can also use Counter from the collections module. A Counter object allows you to easily count the most common elements in an iterable. Note that the Counter object isn't the same as a Python dictionary, but can often be used in the same ways.\n",
    "\n",
    "You can also do arithmetic between Counter objects, which can be useful to add in data from other corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "corpus = \"Arma virumque canō, Trōiae quī prīmus ab ōrīs\\nĪtaliam, fātō profugus, Lāvīniaque vēnit\\nlītora, multum ille et terrīs iactātus et altō\\nvī superum saevae memorem Iūnōnis ob īram\"\n",
    "corpus_list = corpus.split()\n",
    "\n",
    "word_freqs = Counter(corpus_list)\n",
    "\n",
    "print(word_freqs) \n",
    "\n",
    "corpus2 = \"Siquis in hoc artem populo non novit amandi,\\nHoc legat et lecto carmine doctus amet\\nArte citae veloque rates remoque moventur,\\nArte leves currus: arte regendus amor.\"\n",
    "corpus2_list = corpus2.split()\n",
    "\n",
    "word_freqs2 = Counter(corpus2_list)\n",
    "\n",
    "print(word_freqs + word_freqs2)\n",
    "print(word_freqs & word_freqs2) #intersection\n",
    "print(f\"most common word in first corpus {word_freqs.most_common(1)} in second corpus {word_freqs2.most_common(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set\n",
    "tokens = ['this', 'is', 'a', 'test', 'this']\n",
    "unique_tokens = list(set(tokens)) # remember that sets are lists with only unique values\n",
    "\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at and explain some examples from last week's class. The code that follows is from the example of calculating the first part of Delta P, towards the end of the Week 4 notebook. This also includes some examples of list functions and list slicing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = \"temple\"\n",
    "collocate = \"a\"\n",
    "\n",
    "def count_ngram_collocations(x, w1, w2, l_size: int = 1, r_size: int = 1):\n",
    "    lemmata = [t.lemma_ for t in x]\n",
    "\n",
    "    indexes = [i for i, lemma in enumerate(lemmata) if lemma == w1]\n",
    "    cooccurrences = 0\n",
    "\n",
    "    for i in indexes:\n",
    "        left = max(i - l_size, 0)\n",
    "        right = min(i + r_size + 1, len(lemmata))\n",
    "\n",
    "        window = lemmata[left:right]\n",
    "        print(window)\n",
    "        if w2 in window:\n",
    "            cooccurrences += 1\n",
    "            \n",
    "    return cooccurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmata = [t.lemma_ for t in x]  \n",
    "This takes the lemma_ for each token in x, which in this case is a Pandas dataframe, and puts each lemma_ into the variable lemmata\n",
    "\n",
    "indexes = [i for i, lemma in enumerate(lemmata) if lemma == w1]\n",
    "Take the index i for each index, lemma pair in the enumerated lemmata list, if the lemma in question is our node (w1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
